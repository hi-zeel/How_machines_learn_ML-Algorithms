{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45825cb0",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ“˜ Chapter 5: Preparing Data\n",
    "\n",
    "This chapter explains how data are prepared for machine learning: handling missing values, encoding categorical variables, scaling, splitting into train/validation/test sets, and building leak-free preprocessing pipelines. The exposition uses a third-person textbook style with clear LaTeX and runnable code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6fabf6",
   "metadata": {},
   "source": [
    "\n",
    "## 5.1 Importance of Data Preparation\n",
    "\n",
    "Model performance depends strongly on data quality. Effective preparation improves learnability and generalization by ensuring features are informative, comparable in scale, and consistently transformed across training and test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ec6927",
   "metadata": {},
   "source": [
    "\n",
    "## 5.2 Handling Missing Values\n",
    "\n",
    "Real data often contain missing entries.\n",
    "\n",
    "**Strategies**\n",
    "- **Removal**: drop rows/columns containing many missing values (risking information loss).\n",
    "- **Imputation**: replace missing values with a statistic (mean/median/mode) or model-based estimates.\n",
    "\n",
    "**Mean imputation (per feature \\(j\\))**\n",
    "$$\n",
    "x^{(i)}_j \\leftarrow \\frac{1}{\\left|\\{k : x^{(k)}_j \\text{ observed}\\}\\right|} \\sum_{k:\\, x^{(k)}_j \\text{ observed}} x^{(k)}_j.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2599dd0c",
   "metadata": {},
   "source": [
    "\n",
    "## 5.3 Encoding Categorical Variables\n",
    "\n",
    "Most learning algorithms require numerical features.\n",
    "\n",
    "- **Oneâ€‘hot encoding**: creates a binary indicator per category (no ordinal assumptions).\n",
    "- **Ordinal encoding**: maps ordered categories to integers (use only when order is meaningful).\n",
    "\n",
    "**Example (oneâ€‘hot):** Color $(\\in\\{\\text{Red}, \\text{Blue}, \\text{Green}\\}$)  \n",
    "$\\rightarrow$ $([1,0,0], [0,1,0], [0,0,1]$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f45452e",
   "metadata": {},
   "source": [
    "\n",
    "## 5.4 Feature Scaling\n",
    "\n",
    "Algorithms that rely on distances or gradient-based optimization are scale-sensitive.\n",
    "\n",
    "- **Normalization (minâ€“max)**\n",
    "$$\n",
    "x' = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}} \\in [0,1].\n",
    "$$\n",
    "\n",
    "- **Standardization (zâ€‘score)**\n",
    "$$\n",
    "x' = \\frac{x - \\mu}{\\sigma},\n",
    "$$\n",
    "where $(\\mu$) and $(\\sigma$) are the feature mean and standard deviation computed on the **training set only**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48df0e77",
   "metadata": {},
   "source": [
    "\n",
    "## 5.5 Splitting Data\n",
    "\n",
    "To estimate generalization performance and tune hyperparameters, data are partitioned into **training**, **validation**, and **test** subsets:\n",
    "\n",
    "$$\n",
    "D = D_{\\text{train}} \\cup D_{\\text{val}} \\cup D_{\\text{test}}, \\qquad\n",
    "D_{\\text{train}} \\cap D_{\\text{val}} \\cap D_{\\text{test}} = \\varnothing.\n",
    "$$\n",
    "\n",
    "The model is fit on $(D_{\\text{train}}$), tuned on $(D_{\\text{val}}$), and finally assessed once on $(D_{\\text{test}}$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fc6c9c",
   "metadata": {},
   "source": [
    "\n",
    "## 5.6 Pipelines and Data Leakage\n",
    "\n",
    "**Data leakage** occurs when information from outside the training data influences the model fit. To prevent leakage, preprocessing operations (e.g., scaling, encoding, imputation) must be **fitted on the training set only** and then applied to validation/test sets using the learned parameters.  \n",
    "Scikitâ€‘learn **`Pipeline`** and **`ColumnTransformer`** help enforce this discipline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f772811",
   "metadata": {},
   "source": [
    "\n",
    "## 5.7 Handsâ€‘On: Preprocessing the Iris Dataset\n",
    "\n",
    "The following example demonstrates a common workflow: split into train/validation/test, scale numerical features inside a pipeline, and evaluate a classifier. If packages are missing, install them first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db11c2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.9333333333333333\n",
      "Test accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Train/validation/test split: 60/20/20 with stratification\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# All features are numeric in Iris; demonstrate a numeric pipeline.\n",
    "numeric_features = X.columns.tolist()\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', LogisticRegression(max_iter=500))\n",
    "])\n",
    "\n",
    "# Fit on training set only\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Validation accuracy:\", clf.score(X_val, y_val))\n",
    "print(\"Test accuracy:\", clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685b96b5",
   "metadata": {},
   "source": [
    "\n",
    "## 5.8 Summary\n",
    "\n",
    "- Data preparation includes handling missing values, encoding categorical variables, scaling, and careful splitting.  \n",
    "- Proper use of pipelines avoids leakage by fitting preprocessing only on the training set.  \n",
    "- The Iris example illustrated a reproducible preprocessingâ€‘plusâ€‘model pipeline evaluated on validation and test sets.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
